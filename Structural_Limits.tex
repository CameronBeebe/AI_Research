\subsection{The Structural Limits of Artificial Scientific Discovery}

%Generative models can create deep fakes and misinformation, but can the same mechanisms produce scientific discoveries?  What would justification look like for such discoveries?

%If we are equipped with a dialectical architecture, which we have strong epistemological reasons to suppose may lead to interesting novel scientific discoveries, what is the character of such a discovery?  Another way to ask this question is what attractors are described by the phase space of such dialectical systems?



Are Dialectical Models Capable of Scientific Discovery?  What would scaling arguments look like for hierarchical dialectical models?  Would we expect, as available compute and networks scaled, that such systems continuously find usefully novel and efficiently testable scientific theories which are better than current networks of scientists are capable of coming up with?  Or, will there be diminishing returns?  I think the latter is more reasonable to expect, and think that the history of philosophy has important lessons to consider.  Namely, that there is an epistemological limit to testing and adjudicating a set of theories whose terms may be unobservable (in practice or in principle).  Reformulating the theory such that all of the terms are just compounds of observable terms can be thought of as a \emph{structural reduction}.  Many novel `theories' thus correspond to the same structural reduction---and the `knowledge' contained in a model is \emph{really just structural}.

Is it feasible for the goal of AI to be ``maximally truth seeking''?  This implies a \emph{process} or \emph{mechanism} which approaches truth or is less wrong over time.  Underlying this is, arguably, an assumption that AI models should not be interacted with as if they are Oracles.  


%For such limits I suspect that debates around structural realism will have important lessons which transfer over to the discussion about scientific discoveries from generative AI.  

What can be called ``convergent realism'' concerns the debate surrounding whether science (as an iterative process) approaches true reference to real objects in the world---and whether this explains the success of science.  This is related to the No Miracles argument.  Structural realists, on the other hand, would argue that rather than reference to real objects it is reference to \emph{effective structures of relations between (potentially) real objects in the world which are empirically inaccessible or unobservable in practice or in principle.}.  What are we to think of the iterative process of artificial generative models with respect to potential new scientific discoveries? Even if we have a generative model engineered to have an arguably reliable dialectical architecture, what is the nature of the claims?  

The ``intelligence'' of artificial systems is largely derived from learning structural representations of regularities in training data. We could call this ``artificial structural empiricism'': mathematical functions learn to cope with training data primarily by adjusting randomly initialized ``pre-theoretical'' weights.  This coping is parametric in a way that renders unobservable theoretical terms either redundant or obsolete.  (The ``upward path'' to structural realism from Psillos, see e.g. \citep[\S 3]{sep-structural-realism}.)

Can we evaluate such theories on the same footing as other human-derived theories?  Even though we know generative models might be prone to confabulation, but could a dialectical architecture mitigate that worry enough that we would be confident to commit resources to investigating such a theory?  Such commitment of resources would seem to contain the potential to introduce additional accumulative risk from AI into the components of the complex system of civilization we want to make most resilient.


%There are problems with structural realism, and theories formulated purely in structural terms (e.g. via Ramsey sentences).  Structural realism is not uncontroversially accepted, and it does not establish that there is \emph{not} a real (transcendentally ideal) world which exists as a progenitor for empirically accessible structures.  Isomorphisms are cheap, and many different worlds may correspond to the same structure.  [THIS IS WHAT DEMOPOULOS SAYS RIGHT?]


If we are going to use dialectical models in the context of scientific discovery, and we have philosophical justification for their use, presumably this will be because of mechanistic and architectural properties of a certain kind of process.  However, does the epistemological character of the process entail the same about the end result?  Should we expect any kind of robust convergence to a useful (and efficiently checkable) discovery?  

%This reminds of arguments over scientific realism, and whether the process of science itself converges towards true theories which accurately refer.  






%One area where generative models are increasingly being used is in the context of scientific discovery.  

Is it possible that an AI model could learn a different structural representation of the world compared to a typical human biological neural net, and we won’t be able to tell?  If we are going to use artificial generative models in science, what is the epistemological character of apparently new scientific information which is derived from such models?  Assuming it is plausible to suppose some kind of a divide between empirical and theoretical terms, can new information derived from generative AI ever go beyond what philosophers of science call \emph{empirical structural realism}?

A neural network model can learn `empirical' laws which generalize the model's `environment'.  We can say that the model learns an effective bias. That environment is not the same as the one humans like you and I inhabit.   We might suppose, however, that the training data encodes sufficient similarities to our own world.  Sufficient for the model to learn essentially isomorphic empirical laws to our own reality.

A major follow up question is whether the model can learn the same or new \emph{theoretical} laws to our own.  In our explanations of the world we use theoretical laws, which can include potentially non-observable theoretical terms.  They are essential to our cognition of the world.  I think it is extremely unclear whether we should expect current AI models to truly learn anything like our theoretical laws (although they might mimic quite well or be pre-prompted to give the illusion that they understand them).  

To truly `learn' theoretical laws from scratch, what would the training data need to look like, and how do mechanisms in the model `digest' the data?  If we found something which looked like a new theoretical law in the architecture or output of a model, how would we evaluate it?  What tests could we do to find out whether a model has learned different theoretical laws from our own, or new ones?  Would it matter, if the empirical laws remained isomorphic?

Ramsey sentences attempt to do without theoretical terms, so perhaps we could coordinate the machine's epistemology with our own through such a formalization.  See e.g. \citep[\S 26]{Carnap1966} for a philosophical introduction to Ramsey sentences, which has been one central component to the structural realism debate.  A well-known objection to structural realism is from Newman [SEE E.G. DEMOPOULOS].  


%The recent launch of OpenAI's model Sora \citep{Sora2024} brought to light a question about the capability to imitate physics, and whether scene simulators in the future like Sora might be capable to do what traditionally is the domain of persistent 3D game engines and actual physical theories.  Obvious limitations, like in-persistent objects behind occlusions, indicate that a model like Sora does not have a physics world model which is comparable to a 3D game engine, where such persistence is trivially built in.  However, we can imagine a Sora-like model which overcomes many of the current drawbacks---even all of them.  In that case, what is the difference between a 3D game engine and Sora?  Would it even be worth rendering a 3D environment if all that mattered was the \emph{consistent appearance} of a scene?

%Consider a structural description of such a world. Would it differ from the structural description of a 3D rendered world? If for example all rotations of a scene were consistently rendered, and isomorphic to what a 3D engine environment produced, 




  

%The epistemological relationship is beneficial because it facilitates critical thinking methods like the Socratic method, rather than a dependency on something akin to confabulatory divination.  

%One example of what I would classify as a dialectical model is what is known as an adversarial model. Adversarial models in the past (e.g. Generative Adversarial Models, or GANs) relied on a neural network architecture which pitted one ``sub-network'' against another.  One was typically called a Generator, and the other a Discriminator.  This can be used for example to learn an image model that reproduces images close to a training set.  The whole system functions in an iterative loop, such that a Generator tries to fool the Discriminator, and the information of ``failure-to-fool'' is fed back to improve the Generator.  The back-and-forth tension is a reliable feedback loop mechanism.

%I think that we should explore other forms of architecture for dialectical models, in which the internal ``competition'' among sub-nets is composed differently, and not necessarily competitive or describable as ``fooling''.  Rather, sub-nets try to compose the best argument, and other sub-nets try to poke holes.  


%Importantly, as similar to the communicability part of FunSearch, such artificial systems should \emph{transparently display the dialectic between ``adversarial'' sub-nets}.  

%Hierarchical dialectical models (HDMs) represent a potentially novel architecture which I would expect to find in a successful AGI candidate.  If we are going to have AGI, we should try to engineer an architecture which not only out-competes available alternatives, but does so in a theoretically responsible way.  Naturally, what I am describing represents many potential open problems in need of solution, and I am merely speculating that (open source) development on HDMs would be more responsible than other approaches being developed in the open or in secret.  There is the possibility that AGI is dangerous no matter the architecture, and I find it likely we will have multiple competing models.



% \subsection{Relating Learned Structures to Real Structures}

% Helmholtz, Eddington, etc.

% Do effective structures learned correspond to mere adaptive exploits, or actual (causal) world models.  Clearly, absent strong evidence, we should prefer the former.  However, it isn't clear from a complex systems theory point of view what the actual difference would be.

% However, when we consider the phenomenological and idealist views of philosophy, some might think we smuggle back in human specialness, yet I think we should also consider that the real difference can just be understood as knowledge of the scope of different phenomenal worlds. [??]  Different environmental data distributions to adapt to.  

% This is taking the ontology and epistemology of these systems very seriously, just as we would with other creatures. We seem to be able to specify, with a fairly convincing degree of justification, what the environmental data would be like for ants.  

% But, what about silicon hardware implementing linear algebra circuits? [DOES THAT MAKE SENSE?]




%\subsection{The Structural Limits of a Dialectical Process}


\begin{quote}
    Similarly, we have to be careful not to think that it is the ‘struggle’ between a thesis and its antithesis which ‘produces’ a synthesis. The struggle is one of minds; and these minds must be productive of new ideas: there are many instances of futile struggles in the history of human thought, struggles which ended in nothing. And even when a synthesis has been reached, it will usually be a rather crude description of the synthesis to say that it ‘preserves’ the better parts of both the thesis and the antithesis. This description will be misleading even where it is true, because in addition to older ideas which it ‘preserves’, the synthesis will, in every case, embody some new idea which cannot be reduced to earlier stages of the development. In other words, the synthesis will usually be much more than a construction out of material supplied by thesis and antithesis. Considering all this, the dialectic interpretation, even where it may be applicable, will hardly ever help to develop thought by its suggestion that a synthesis should be constructed out of the ideas contained in a thesis and an antithesis. 
    \citep{PopperCR1963}
\end{quote}



If one is convinced that the epistemic stance of AI is something like epistemic structural realism, it raises an important question about why human agents do (or should) have a different stance.  The machines are structuralists.  Probably epistemic structuralists.  But are they realists, or pragmatic and adaptive structuralists?  What about humans?


Ultimately, I see the problem of how to evaluate  scientific discovery from generative models (especially in areas like fundamental physics) as roughly falling into debates over convergent realism and structural realism.  A dialectical system may provide immediate benefits, or mitigate limitations of current generative AI.  However, the long term widespread use may result in diminishing or even negative returns when the trajectory of the entirety of human civilization is considered.  Our use of dialectical systems might lead to structural theoretical constructs which contain terms which at best do no better than human derived theories.  At worse, we might be inundated with a flood of non-efficiently testable or unfalsifiable theories.  This is a point arguably stemming from Kant, and Popper's reinterpretation:

\begin{quote}
    If I were to give some sort of modernized reconstruction, or reinterpretation, of Kant, deviating from Kant’s own view of what he had done, I should say that Kant showed that the metaphysical principle of reasonableness or self-evidence does not lead unambiguously to one and only one result or theory. It is always possible to argue, with similar apparent reasonableness, in favour of a number of different theories, and even of opposite theories. Thus if we get no help from experience, if we cannot make experiments or observations which at least tell us to eliminate certain theories—namely those which although they may seem quite reasonable, are contrary to the observed facts—then we have no hope of ever settling the claims of competing theories.
    \citep[p. 439]{PopperCR1963}
\end{quote}

%The situation is the same we have been in with respect to human derived theories, but at least we might give an argument why we should dignify a human's unfalsifiable theory.  Perhaps we respect the phenomenological experiences of fellow humans like ourselves.  But generative models `live' in a derivative world.  Their phenomenological `experiences' are second order compared to our own.  These models do not have experiences like ours with which to ground their theories.  



There theories may be in principle falsifiable, but we should also consider whether there might be sophisticated components of a generative theory (linguistic, mathematical, or otherwise) which exceed the capacity of networks of scientists to efficiently evaluate.  I argue such a potential situation represents a risk from generative AI, and it may happen in an accumulative manner as outlined in \citep{Kasirzadeh2024WIP}.  


%\subsection{Stuff to Move or Remove}








%There are many views in the history of philosophy which pertain to dialectical mechanisms.  Many will be familiar with dialectics in rhetoric, argumentation theory, and epistemology.  Daoism is arguably a metaphysical dialectical view of reality.  Later views (e.g. Hegel and Marx) considered also material and metaphysical processes.  While I'm aware of the very broad scope of such ideas, the focus of my project will be on a structural systems theory characterization of a dialectical system.  










%[ISNT THIS REDUNDANT WITH ABOVE]
%Crucially, I argue that these structures also embody certain important philosophical principles which ground the thesis that dialectical AI models might represent a significant baseline for addressing many well known issues in machine learning and AI.  Consider the following list of desiderata:


% \begin{itemize}
%     \item Interpretability
%     \item Explainability
%     \item Factual Accuracy
%     \item Bias
% \end{itemize}

%Current widespread LLMs, for example, have trouble with these basic requirements, not to mention other nuanced issues with confabulations, safety, and other ``fallacies'' which might derive from the above.  How might one design an alternative internal architecture (mathematical functions and computational mechanisms) capable of systematically protecting against such issues?  

%Dialectical models should be used dialectically.  In order to do so, the interlocutors must be on a similar leve














%[GAME THEORY HERE]





%At this point I'm sure many philosophers might already find it understandable why it might make sense to refer to such an architecture as ``dialectical''.  Alas, as far as I can tell, such a connection was first described in a technical paper.







%I will argue that philosophers of science might be more interested in the sort of interpretability they are capable of compared to alternative architectures.  Because of their architecture, they have the obvious ability to internally compensate or error-correct, if designed properly.  Once a common kind of error is known, e.g. confabulation aka hallucination in LLMs, a dialectical LLM would have an architecture where there is an internal adversarial sub-component.







 





%We might expect that innovation on the simple GAN architecture could overcome 






%I suspect that, if we are to engineer an artificial context of discovery, we would need to introduce some source of disturbance such that there is no stable equilibrium, and thus the players are forced to adapt dialectically towards a new attractor.  This disturbance could come from human interactions with the model.  [DOES THAT MAKE SENSE?]















%Dialectical models have the potential to mitigate many of the ethical and epistemological issues currently present in widely used generative models.   By the term  \emph{dialectical} I mean to include several different yet related kinds of artificial neural network architectures, where the architectures have a certain kind of structure (or cluster of related structures).  These structures embody certain cybernetic or control theory principles concerning information flow and feedback loops, which we can understand fairly straightforwardly from a birds eye view using the tools of abstract phase spaces, attractors, and simple control diagrams.  







%Another is mitigating bias, but in a different way than other approaches which seek to balance bias---or even create ideal ``unbiased'' models.  I find these approaches problematic, since 



%Even if everyone agreed on criteria for an ``unbiased'' model, and everyone agreed to use it, there are fundamental epistemological limitations about whether we will be able to understand when the models are confabulating or nudging in nuanced ways that affect influential actors in society, which in turn affect the trajectory of science or politics in our civilization.


%[CHECK DIALECTICAL GANS PAPER: DO THEY GO BEYOND HEGELIAN IDEA I.E. Open debate ala JSM etc.]


%As a philosopher, we might explain why certain of these architectures overcome limitations and improve on important metrics because they are \emph{beneficially dialectical}.  A beneficially dialectical architecture amounts to a game between components in a machine learning model which lead to equilibrium where undesirable features are minimized.  I am not aware of a comprehensive formal philosophical treatment of this subject, which could prove useful for providing ethical guidance and regulation in AI industry.  While such would be outside the scope of a limited postdoc, my preliminary investigation could I think lead to grant applications on the topic.




%If humans interact with generative models like LLMs as if they are AI Oracles, they become more of a passive part of a  knowledge feedback loop.  Like lazily asking a teacher for the answer to a question, where effort and critical thinking and agency in the world would lead to a more satisfying embodied answer.  Reading a book and doing research and engaging one's own biological neural network has e.g. neural, neuro-chemical, and emotional value above and beyond the critical thinking skills as well.

%Not to mention that AI Oracles which crawl the internet for answers are encountering a more polluted environment as they try to find seemingly authoritative and genuine facts.  Generative models, and human agents who are proliferating their outputs, are polluting the very data set which would make them useful long term.  Consider an analogy with \textbf{a microphone getting too close to a speaker.}  Garbage in, garbage out.

%While much of this seems obvious to me after a great deal of thought the last few years, I'd be interested in hearing counterarguments.  These models aren't going away anytime soon, and banning them will not work as open source models are proliferated and competitive with closed source ones.  

%\textbf{I think it would be worthwhile to explore whether we might be able to develop an alternative kind of generative model which encourages non-Oracle use and critical thinking by the human user.  It outputs considered dialectic from multiple biased models.} \\

%\textbf{POTENTIAL CRITICISM}: One can ask current models to simulate a dialectic, or just fine tune on philosophical dialogue texts, so would we really need some new architecture for specifically dialectical models? [REMOVE THIS?]

%\textbf{POTENTIAL RESPONSE}: 

%There is no free lunch.  \cite{Wolpertetal1997}  Effective models learn bias.  \cite{Mitchell1980}  

%Maybe even some computational efficiency to go down the route of a cluster of adversarial and biased models. They might also be able to be distributed better on compute devices. 

%[THATS ACTUALLY WHAT FUNSEARCH AND MIX OF EXPERTS DO AND THERE ARE COMPUTATIONAL BENEFITS]




%Community Notes is a feature on X (Formerly Twitter) which uses a model based on history of disagreement between users in order to improve fact checking.  Jensen Huang suggested a model of AGI where there is a super-model taking into account a committee of sub-models who interact to achieve the super-model's goal (determined via a user). These are also dialectical architectures.



%Non-fact confabulations, where there is no agreed upon ground truth (controversial confabulation) or worse, where there is no answer that any human has thought of which could be considered a fact (perhaps due to the nature of the question, etc.)


