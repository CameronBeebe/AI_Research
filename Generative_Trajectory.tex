\subsection{Generative Confounds and the Trajectory of Science}

%After more than a decade of widespread social media use, we now have some idea how populations of humans can be affected by algorithmic content pushing.  The algorithms are driven by feedback loops on an industrial scale.  Supercomputers (petascale and, now, exascale) are employed in order to do large amounts of mathematics (conventionally matrix multiplication on forward passes and the chain rule backpropagating feedback signals) in the optimization process.  The information contained in the reinforcing signals from large populations of human consumption allows social media companies to adjust their algorithms, and make them more optimal with respect to the goal of keeping users neurochemically hooked and coming back for more content, and to serve more ads.  

%Increasingly, however, social media companies are ostensibly trying to introduce what I think can be appropriately called \emph{nudges}, in consistent terminological usage with the theory of nudging as put forth in Sunstein et al.  The overwhelming criticism and pushback these companies receive when hateful content or misinformation is algorithmically pushed is almost universal.  Facebook, for example, has employed \emph{tens of thousands} of employees to try to manually flag and determine what should or should not be pushed, in what seems an obvious attempt to create a labeled dataset to train algorithmic content moderation models.

%My understanding of nudging which I think seems fair goes something like this: when all fair choices are presented to an agent (including the ``right'' choice according to some economic or political theory) the probability of the specific (theory-desired) choice might increase at the population level.  This is called ``choice architecture'', and I take it to be an ideal formulation like other theoretical frameworks in economics or related sciences.  

%One problem with algorithmically implementing such an idea is not just that the programmers of the algorithm may have implicit bias (which may or may not agree with the user's implicit bias).  This would, for example, affect the space of choices in certain ways.  However, a much more problematic issue I take to be that users are already in a vulnerable state, being targeted for their attention via the content optimization loop, and therefore susceptible to two kinds of cognitive responses.  

%\noindent\textbf{Positive:} \emph{uncritically believing in the implicit biases of the choice architects as a worldview, which we can perhaps view as a form of confirmation bias}.

%\noindent\textbf{Negative:} \emph{believing that the choice architects are maliciously projecting their world view (which clashes with the user's)}

%[nonsense?]

%I would compare this to 



%So, on Oracular calls to the composed network, a dialectic is displayed.  Additionally, one should be able to call upon a ``committee'', and the function of the model is not general but specific to a critical methodology.  




%However, for quite some time now, I have been concerned with what I would call \emph{systemic confounds} from generative AI models.  These confounds are present in the results shown from popular search engines, for example ChatGPT output text copied and pasted onto popular forums like StackOverflow and Reddit, and shown by Google search.  This is the obvious ``pollution'' confound, but I believe there are much more important confounds from generative models we should focus on for the methodology of science.




%Social media content optimization algorithms have for some time now affected large populations of humans.  Even small changes in content, distributed widely, are expected to potentially affect the views of individuals within the population.  I think it is fair to consider in a similar context a new class of public-facing \emph{generative} algorithms.  The most widely known is OpenAI's large language model (LLM) called ChatGPT.

%With the advent and widespread availability and use of generative LLMs, there is now a new optimization loop which reinforces through human feedback a different class of models.  These models aren't necessarily optimized in order to serve content, but 










Consider a phase space, in which each point represents the total state of every human with agency, and every relevant factor of human civilization.  This complex system has, and will have, a \emph{trajectory} in phase space.  In this space there might be `valleys' and `peaks' (attractors and repellors) which represent the grain or currents due to various mechanisms and processes, incentives and disincentives, positive and negative feedback loops, and structural constraints in the world of the system.  What constraints or influences do generative models impart on such an abstract space?

Modern large language models, such as ChatGPT, are widely available and used by millions of humans.  These models are optimized to create linguistic content that impresses in some way a population of human users.  A hardware and software arms race is rapidly increasing the dedicated and specialized compute to training and improving these models, and also to serve artificial linguistic content to large populations of humans.  These models are improving rapidly not just via reinforcement learning from human feedback (RLHF), but also from architectural innovations in the structure of deployed models.  Such a rapid deployment of new technology of course raises many ethical concerns, but also existential risks.

%When considering the risk landscape, I agree with e.g. Anil Seth and many others when I have \emph{extreme} doubt about any sort of sentience or consciousness in mathematical models implemented on even exascale supercomputers.  Given a distribution over possible things to worry about with respect to artificial intelligence, it should be low priority to think about if/whether/when these models might become conscious.

%Rather, we should focus more on how humans interact with these models.  Turing Tests are best understood as a measure of the ability of an artificial model to \emph{trick} humans.  Modern models, like LLMs, are trained not just on a data set, but refined through reinforcement learning from human feedback. In some very real sense, many of these models are reinforced to impress both individual humans as well as populations of humans.  We should be far more concerned with potential populations of tricked humans than an artificially sentient trickster.    


Atoosa Kasirzadeh makes a convincing case for what she calls the \emph{accumulative} risk of AI. \citep{Kasirzadeh2024WIP}  It seems to me that central to this accumulative risk is the relationship large populations of humans have to widely available generative artificial systems.  Individual users as well as concentrated or disproportionately accumulating use in influential (scientific) networks are of particular concern.  Small mistakes and misuses can add up: 

\begin{itemize}
    \item bugs in generated code
    \item relying on generated summaries of articles never read
    \item using generated text in scientific articles
    \item using generative models to influence `novel' scientific theory or results at the fringe of a particular scientist's understanding or, worse, 
    \item at the fringe of a majority of a field's understanding---in which case, an error may persist depending on testing and replication resource complexity
\end{itemize}

So, if we want to foster \emph{resilience}, it seems to me that we need to focus on making resilient individuals, since they are presumably the primitive  `unit' or subsystem relevant to the coherent functioning of an interconnected global complex system.  The more resilient individuals are, the more robust  larger subsystems like local organizations and municipalities will be.  Turing Tests are best understood as a measure of the ability of an artificial model to \emph{trick} humans.  Modern models, like LLMs, are trained not just on a data set, but refined through reinforcement learning from human feedback. In some very real sense, many of these models are reinforced to impress both individual humans as well as populations of humans. 

I agree with e.g. Anil Seth and many others when I have \emph{extreme} doubt about any sort of sentience or consciousness in mathematical models implemented on exascale supercomputers.  Given a distribution over possible things to worry about with respect to artificial intelligence, it should be low priority to think about if/whether/when these models might be `conscious'.  But a substantial slice of a large population of humans who are interacting with these models might get tricked in various different ways (including tricked that the system is conscious...)

Furthermore, it seems obvious that some networks of individuals (and associated institutions) are more influential and capable of being robust.  Namely, scientific networks and institutions.  Not just are scientific networks more capable, but I argue it is more important for the whole system that there are resilient scientific networks (as opposed to, say, political or corporate networks).  These are also the networks with individuals most likely to have developed critical thinking skills able to resist poor usage of AI models.  Unfortunately, the current trend looks poor, as scientists (with institutional and publishing incentives) appear to be among the first and most frequent users of novel AI models, in particular generative language models.  There are already many examples of misuse by scientists.\footnote{\href{https://www.404media.co/scientific-journals-are-publishing-papers-with-ai-generated-text/}{Article from 404 Media.}}

Specialized and narrow models (e.g. for predicting turbulent flow) are of a wholly different character from large language models (LLMs).  The latter affect many aspects of the work flow of scientists, and arguably the critical thinking process of reading and writing and forming arguments.  Additionally, if we are going to use generative models like LLMs for scientific discovery in areas like fundamental physics (or in borderline areas which may be considered metaphysics by some), there are major resource costs to checking and testing generative claims.  Time and money.  A similar `pollution' point also affects the economic trade offs for peer review.  Generative models, if not checked, are beginning to flood the zone of critical intellectual effort.\footnote{My methodology has changed in response: prioritizing the synthesis of historical works in philosophy.} \footnote{\href{https://twitter.com/BeebsMemes/status/1759282741680443600}{Post on X} discussing a recent interesting case of a student who (mis-)used LLM models in their work.}

%What philosophical views do scientists and developers have, and what epistemological assumptions do they make about AI models?  When one reads, for example, the scientific treatises of some of the smartest people of the twentieth century, there is an astonishing amount of philosophical sophistication.  Much of this has to do with the educational system in Germany and Austria.  But the point is that \emph{it is popular for scientists to forego the philosophy in favor of results and publication}.  The incentives make it so.

%Atoosa also suggests simulations and modeling of the complex global system, which I wholeheartedly endorse and would like to explore as well.  



%[MOVE]

%We need to develop checks and balances, proper-use directives, and standard operating procedures for the use of generative language models in science.



%Perhaps using translation specific language models should be allowed under certain guidelines, especially ones vetted and known to be robust by scientific administrators. So students can write in their comfortable native tongue… and there would be less likelihood of spurious or incoherent content being introduced into the academic process.

%But this sidesteps the more fundamental issue which is that critical thinking skills in human neural nets are atrophying as people become dependent.  I think this student should \emph{not} have used the LLM to rewrite anything, especially if it was something they were unsure about in a second language to begin with. 

%As long as there was an affidavit of human authorship, and/or affidavit of translation only (e.g. according to model and version \#), I think it would have been an arguably okay use of AI tools for academic work. But anything above translation gets very suspect, for reasons having to do with the human values of science and the goal to train critical human thinkers in academia.

%There is a new and extremely dominant influence in academia, and incentives to empower that influence will lead to outcomes inconsistent with the purpose of higher education if we don’t start figuring out healthy ways to use the technology.